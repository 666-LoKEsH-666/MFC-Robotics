<!DOCTYPE html>
<html lang="en">
<head>
  <!-- generated by AI website-builder prompt -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Speech-to-Handwriting Robot - Methodology & Implementation</title>
  <link rel="stylesheet" href="css/style.css">
</head>
<body>
  <header class="header">
    <nav class="navbar container">
      <div class="logo"><a href="index.html">MFCxRobotics</a></div>
      <ul class="nav-menu">
        <li class="nav-item"><a href="index.html" class="nav-link">Abstract</a></li>
        <li class="nav-item"><a href="intro.html" class="nav-link">Introduction</a></li>
        <li class="nav-item"><a href="team.html" class="nav-link">Team</a></li>
        <li class="nav-item"><a href="toc.html" class="nav-link">Contents</a></li>
        
        <li class="nav-item"><a href="literature.html" class="nav-link">Literature</a></li>
        <li class="nav-item"><a href="method.html" class="nav-link active">Methodology</a></li>
        <li class="nav-item"><a href="results.html" class="nav-link">Results</a></li>
        
        <li class="nav-item"><a href="conclusion.html" class="nav-link">Conclusion</a></li>
        <li class="nav-item"><a href="references.html" class="nav-link">References</a></li>
      </ul>
      <div class="hamburger"><span class="bar"></span><span class="bar"></span><span class="bar"></span></div>
    </nav>
  </header>

  <main class="main-content">
    <div class="container">
      <div class="content-card">
        <h1 class="section-title">Dobot Magician: Kinematics, Control, and Integration</h1>

        <h3>Introduction to DOBOT Magician</h3>
        <p>
          The <strong>DOBOT Magician</strong> is a compact, user-friendly 4-DOF articulated robotic arm designed for education, research, and light industrial applications. It supports a wide variety of functions including 3D printing, laser engraving, drawing, and writing — making it highly versatile for prototyping and learning environments.

          In this project, the Dobot is used for robotic handwriting, where it physically writes words recognized from spoken input. Understanding how the robot moves requires an in-depth look at its kinematics, degrees of freedom, motion control, and how these are integrated via programming.
          
        </p>
        <div style="display: flex; justify-content: center; margin-top: 20px;">
            <img src="uploads/image3.jpg" style="width: 48%;">
        </div>
          

        <h3>Basic Structure and Specifications</h3>

<table>
  <thead>
    <tr>
      <th>Joint</th><th>Function</th><th>Range</th><th>Max Speed</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>Joint 1</td><td>Base Rotation (θ₁)</td><td>–90° to +90°</td><td>320°/s</td></tr>
    <tr><td>Joint 2</td><td>Rear Arm Rotation (θ₂)</td><td>0° to +85°</td><td>320°/s</td></tr>
    <tr><td>Joint 3</td><td>Forearm Rotation (θ₃)</td><td>–10° to +95°</td><td>320°/s</td></tr>
    <tr><td>Joint 4</td><td>End-Effector Roll (θ₄)</td><td>–90° to +90°</td><td>480°/s</td></tr>
    <tr><td>Z-axis</td><td>Linear vertical motion</td><td>Prismatic</td><td>--</td></tr>
    <tr><td>Payload</td><td>Max load capacity</td><td>250g</td><td>--</td></tr>
  </tbody>
</table>

<p>The robot includes modular end-effectors, allowing a pen to be mounted for drawing. The device is connected and controlled via the Dobot Python SDK, which abstracts the underlying kinematic computations.</p>

<!-- Two images side by side -->
<div style="display: flex; gap: 20px; margin-top: 20px;">
  <img src="uploads/image1.jpg"  style="width: 48%;">
  <img src="uploads/image2.jpg" style="width: 48%;">
</div>

        <h3>Degrees of Freedom and Kinematic Model</h3>
        <p>The kinematic structure is an RRR + P system (Revolute–Revolute–Revolute + Prismatic), ideal for horizontal plane writing with vertical motion control. </p>
        <ul>
          <li><strong>Base (θ₁):</strong> Horizontal rotation</li>
          <li><strong>Shoulder (θ₂):</strong> Vertical lift</li>
          <li><strong>Elbow (θ₃):</strong> Arm bending</li>
          <li><strong>Wrist (θ₄):</strong> Pen rotation</li>
          <li><strong>Z-motion:</strong> Pen up/down</li>
        </ul>

        <h3>Forward Kinematics</h3>
<p>
  Forward kinematics (FK) calculates the <em>Cartesian coordinates</em> <strong>(x, y, z)</strong> of the end-effector, given joint angles <strong>(θ₁, θ₂, θ₃)</strong>. The Dobot’s arm resembles a <em>planar 2-link manipulator</em>, which is modeled as:
</p>

<pre class="diagram">
x = L₁ * cos(θ₁) + L₂ * cos(θ₁ + θ₂)
y = L₁ * sin(θ₁) + L₂ * sin(θ₁ + θ₂)
z = z  (directly controlled)
</pre>

<p>
  Where:
</p>
<ul>
  <li><strong>L₁</strong>, <strong>L₂</strong> = link lengths (upper arm and forearm)</li>
  <li><strong>θ₁</strong>, <strong>θ₂</strong> = joint angles for shoulder and elbow</li>
  <li><strong>x, y</strong> = position of the end-effector in the horizontal plane</li>
  <li><strong>z</strong> = vertical height (used to lift or lower the pen)</li>
</ul>

<h3>Denavit–Hartenberg Transformation</h3>
<p>
  The transformation from base to end-effector for each joint can be modeled using the standard <em>DH parameter matrix</em>:
</p>

<pre class="diagram">
T(i) =
| cos(θᵢ)  –sin(θᵢ)   0   aᵢ |
| sin(θᵢ)   cos(θᵢ)   0    0 |
|    0         0      1   dᵢ |
|    0         0      0    1 |
</pre>

<p>
  Where:
</p>
<ul>
  <li><strong>θᵢ</strong> – Joint angle (rotation)</li>
  <li><strong>aᵢ</strong> – Link length (distance along X)</li>
  <li><strong>dᵢ</strong> – Link offset (distance along Z)</li>
</ul>

<p>
  The total transformation from base to end-effector is given by:
</p>

<pre class="diagram">
T_total = T₁ × T₂ × T₃ × T₄
</pre>

<p>
  The resulting end-effector position <strong>(x, y, z)</strong> is extracted from the final transformation matrix’s translation column.
</p>

<h3>Inverse Kinematics</h3>
<p>
  Inverse Kinematics (IK) calculates the <strong>joint angles</strong> required to reach a desired end-effector position <strong>(x, y, z)</strong>. This is the reverse of Forward Kinematics and is essential when defining target positions for tasks like handwriting.
</p>

<h4> Analytical IK (Used Internally in Dobot Firmware)</h4>

<p><strong>Step 1 – Compute Distance D:</strong></p>
<pre class="diagram">
D = (x² + y² – L₁² – L₂²) / (2 * L₁ * L₂)
</pre>

<p><strong>Step 2 – Elbow Angle θ₂:</strong></p>
<pre class="diagram">
θ₂ = arccos(D)
</pre>

<p><strong>Step 3 – Shoulder Angle θ₁:</strong></p>
<pre class="diagram">
θ₁ = atan2(y, x) – atan2(L₂ * sin(θ₂), L₁ + L₂ * cos(θ₂))
</pre>

<p>
The <strong>Z</strong> (vertical pen height) and <strong>R</strong> (end-effector rotation) are controlled independently and directly.
</p>

<p>
These calculations are handled internally by the Dobot SDK/firmware, so you typically do not have to compute joint angles manually.
</p>

<p><strong>Example code using the Dobot Python SDK:</strong></p>
<pre class="diagram">
device.move_to(x=210, y=30, z=-68, r=0)
</pre>
<p>
This single line internally triggers the inverse kinematics engine and moves the robotic arm to the specified (x, y, z, r) coordinates.
</p>

<h3>Jacobian and Numerical Methods (Advanced IK – Not Used Directly)</h3>

<p>
  In scenarios where <strong>real-time joint velocities</strong> are required—such as force feedback, compliant motion, or adaptive writing pressure—the <strong>Jacobian matrix</strong> <em>J(θ)</em> becomes essential.
</p>

<p>
  The Jacobian maps joint velocities to end-effector linear velocities:
</p>

<pre class="diagram">
ẋ = J(θ) · θ̇
</pre>

<p>
  Where:
</p>
<ul>
  <li><strong>ẋ</strong> – End-effector velocity vector (dx/dt, dy/dt, dz/dt)</li>
  <li><strong>θ̇</strong> – Joint velocity vector (dθ₁/dt, dθ₂/dt, ...)</li>
  <li><strong>J(θ)</strong> – Jacobian matrix as a function of current joint angles</li>
</ul>

<h4>Solving for Joint Velocities</h4>

<p><strong>1. Pseudoinverse Method:</strong></p>
<pre class="diagram">
θ̇ = J⁺(θ) · ẋ
</pre>
<p>This provides a least-squares solution when the system is redundant or underdetermined.</p>

<p><strong>2. Transpose Method:</strong></p>
<pre class="diagram">
θ̇ = J(θ)<sup>T</sup> · ẋ
</pre>
<p>
  Common in torque-based or energy-efficient control, the transpose method is simpler but less precise in redundant systems.
</p>

<p>
  These methods are <strong>computationally intensive</strong> and are generally <strong>not required</strong> for pre-defined trajectory tasks like handwriting, where positions are known in advance and motion can be planned offline.
</p>

<h3>Integration in This Project</h3>
<p>
  Each letter stroke is converted to (x, y, z) coordinates. For example, to draw the letter <code>'L'</code>:
</p>
<pre class="diagram">
draw_line(dobot, x, y, x, y - 20)
draw_line(dobot, x, y - 20, x + 10, y - 20)
</pre>
<p>
  The robot’s controller handles inverse kinematics (IK) and sends joint-level commands to the actuators.
</p>
<ul>
  <li><strong>Pen Down:</strong> z = –68</li>
  <li><strong>Pen Up:</strong> z = –60</li>
</ul>
<p>
  <strong>Drawing Direction:</strong> Writing progresses primarily along the Y-axis (forward), with X-axis motion for stroke shapes.
</p>






<h1 class="section-title">Speech-to-Text Conversion</h1>

<h3>Overview</h3>
<p>
  The speech recognition module plays a crucial role in our system by converting spoken commands into text. This offline approach uses a Deep Neural Network (DNN) combined with Hidden Markov Models (HMMs) to provide accurate, real-time transcription of audio input. The output is then used to direct the Dobot Magician for robotic handwriting.
</p>

<h3>Step 1: Audio Input and Framing</h3>
<p>
  The audio is sampled at <strong>16,000 Hz</strong>, split into frames of <strong>25ms</strong> each using an overlapping sliding window. This overlap helps maintain continuity between frames and increases recognition accuracy.
</p>
<div style="margin-top: 20px;">
    <img src="uploads/equation1.jpg" style="width: 40%; margin-bottom: 20px;">
    <img src="uploads/equation2.jpg" style="width: 60%;">
  </div>
  
  

<h3>Step 2: Feature Extraction Using MFCCs</h3>
<ul>
  <li><strong>Windowing:</strong> Reduces spectral leakage using functions like Hamming.</li>
  <div style="display: flex; justify-content: center; margin-top: 20px;">
    <img src="uploads/equation7.jpg" style="width: 23%;">
  </div>
  <p>x[n]: original audio signal in frame</p>
   <p> w[n]: window function (like Hamming or Hann)</p>
    <p>x̂[n]: windowed signal ready for FF</p>
  <li><strong>Fourier Transform:</strong> Converts the time-domain audio signal into frequency-domain.</li>
  <div style="display: flex; justify-content: center; margin-top: 20px;">
    <img src="uploads/equation6.jpg" style="width: 23%;">
  </div>
  <li><strong>Mel Filterbank:</strong> Mimics how humans perceive sound frequencies.</li>
  <div style="display: flex; justify-content: center; margin-top: 20px;">
    <img src="uploads/equation3.jpg" style="width: 23%;">
</div>
  <li><strong>Log Energy:</strong> Represents perceived loudness on a logarithmic scale.</li>
  <div style="display: flex; justify-content: center; margin-top: 20px;">
    <img src="uploads/equation4.jpg" style="width: 23%;">
</div>
  <li><strong>Discrete Cosine Transform (DCT):</strong> Compresses the data, retaining the most informative coefficients.</li>
</ul>
<div style="display: flex; justify-content: center; margin-top: 20px;">
    <img src="uploads/equation5.jpg" style="width: 23%;">
</div>

<h3>Step 3: Acoustic Modeling</h3>
<p>
  The MFCC vectors are input into a <strong>Deep Neural Network (DNN)</strong> trained to classify phonemes and acoustic units. This helps transform raw audio features into a higher-level representation suitable for decoding.
</p>

<h3>Step 4: Decoding with HMM + Viterbi + Language Model</h3>
<p>
  The system models words as sequences of HMM states. The <strong>Viterbi algorithm</strong> is used to determine the most likely state sequence (i.e., what was said) based on the acoustic input.
</p>
<div style="display: flex; justify-content: center; margin-top: 20px;">
    <img src="uploads/equation8.jpg" style="width: 23%;">
  </div>
<ul>
  <li><strong>O:</strong> Observed feature vectors (from MFCCs)</li>
  <li><strong>Q:</strong> Sequence of phoneme states</li>
  <li><strong>W:</strong> Word sequence (final output)</li>
</ul>
<p>
  A <strong>language model</strong> scores the word sequences, favoring more linguistically probable outputs.
</p>

<h3>Step 5: Final Scoring and Output</h3>
<p>
  The final word output maximizes a combined score:
</p>
<pre class="diagram">
Score = P(O | W) * P(W)^λ
</pre>
<ul>
  <li><strong>P(O | W):</strong> Acoustic model likelihood</li>
  <li><strong>P(W):</strong> Language model likelihood</li>
  <li><strong>λ:</strong> Tunable weight factor to balance both components</li>
</ul>

<p>
  This step ensures the most likely and coherent transcription is selected from the speech input, ready to be passed into the robotic drawing pipeline.
</p>


        <div class="text-center mt-3">
          <a href="results.html" class="nav-link">→ Continue to Results and Discussion</a>
        </div>
      </div>
    </div>
  </main>

  <footer class="footer">
    <div class="container">
        <p>Speech-to-Handwriting Robot</p>
    </div>
</footer>

  <script src="js/script.js"></script>
</body>
</html>
